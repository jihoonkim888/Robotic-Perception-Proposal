\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{mathptmx} % change default font to times
\usepackage[a4paper, margin=1in]{geometry}
\usepackage{graphicx}
% \usepackage[UKenglish]{babel}
% \usepackage{csquotes}
\usepackage[numbers]{natbib}
\usepackage{lscape}
\usepackage{hyperref}


\title{An Explainable Robotic Perception Framework for Indoor Scene Understanding}
\author{Jihoon Kim}
\date{}

\begin{document}

\maketitle

\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Background and Problem Description}
%%% A description of the topic and an explanation of why further research in the area is important (the gap in the literature - what we need to know)
%%% A key question, hypothesis or the broad topic for investigation

% Why is indoor scene understanding important?
The field has witnessed rapid improvements in scene understanding thanks to the introduction of state-of-the-art deep learning methods and ever-improving computational resources.
AI with a good scene understanding can be useful in many places.
%(e.g., domestic robots, autonomous vehicles).
For example, it may help people with visual difficulties by creating an accurate 3D representation of the surrounding environment and providing safe guidance. Also, the recent rise of \textit{``metaverse''} further increases the demand of scene understanding for real-time communication and interaction between its users and the real-world environment \citep{oculus2021}.

However, 3D scene understanding still remains a very challenging task till this date.
This is mainly due to interactions between objects and changes in information across different scenes \citep{naseer2018indoorsurvey}.
Also, there are fundamental differences in the ways humans and machines perceive the environment.
Humans are naturally capable of understanding complex relationships between objects and their semantics from an image or a video.
However, these are simply a list of numeric values for machines.
Therefore, a machine learning agent must be able to extract both geometric and semantic information in order to provide useful knowledge to humans.

The house interior industry may also hugely benefit from the rise of scene understanding.
Providing a 3D replica of a real house with editable objects can be hugely attractive to its customers, as they will be able to easily change and customise their houses as wished with low costs in a virtual environment.
However, to the best of my knowledge, there has been very little study conducted in this area.
Therefore, it will be an interesting and contributable work to develop an entire framework for full indoor perception with robots as a doctorate study.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Aim, Objectives and Contributions}
%%% An outline of the key aims of the research
This research aims to develop a real-time robotics perception framework that constructs a 3D representation of a real-world scene with editable geometric models.
The objectives are to:
\begin{enumerate}
    \item develop a real-time robotic perception and navigation method that captures information from real-world indoor scenes with explainable deep learning methods
    \item develop an automated 3D scan to CAD method that uses captured information to reconstruct a scene and convert into editable geometric models with semantic contexts with minimum human interventions
\end{enumerate}

% 1) develop a real-time robotic perception and navigation method that captures information from real-world indoor scenes with explainable deep learning methods; and
% 2) develop an automated 3D scan to CAD method that uses captured information to reconstruct a scene and convert into editable geometric models with semantic contexts with minimum human interventions.

% anticipated research contributions
% there are conceptual, methodological, theoretical (most of the time) contributions
Theoretical and methodological contributions in indoor scene understanding with robotic perception will be made to the relevant field by achieving these objectives.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Literature Review}
\label{sec:literature-review}
%%% evidence of interest in, experience of, and understanding of the proposed area of study
%%% A brief outline of key literature in the area [what we already know]
\subsection{Perception}
% scene classification, object detection
\subsubsection{Image Classification}
% Section 6.1 of Naseer et al.
A perception framework should perform scene image classification to give the object detection and semantic segmentation models more general and useful information about a scene.
% Information about a scene or objects is fundamental for more sophisticated tasks such as object detection and scene segmentation.
Image classification is a basic, yet very important and challenging task for scene understanding.
For example, perceived data must be represented with the best method (e.g., point cloud, 3D mesh) to maximise the useful information while minimising the computational complexity.
Further, the proposed framework must be able to work well with limited amount of data, 3D deformations and background clutter that occur in the real-world conditions.
\citet{socher2012convolutional} introduced the use of Recurrent Neural Network (RNN) with convolutional layers to extract low and high-level features from the perceived RGB-D data and improve the classification performance.
Further, \citet{wu20153d} proposed the use of convolutional Deep Belief Network (DBN) that detects patterns from unlabelled data and showed that DBN can be used to learn joint distributions from 3D voxels.
The use of convolutional layers reduces the computational costs compared to fully-connected (FC) layers thanks to lower number of parameters from weight sharing.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{images/mvcnn.png}
    \caption{Multi-View CNN proposed by \citet{su2015multi}}
    \label{fig:mvcnn}
\end{figure}

\citet{su2015multi} proposed Multi-View CNN (MVCNN) that uses multiple 2D render views of 3D objects for classification, shown in Figure \ref{fig:mvcnn}.
It would be possible to capture multiple images of a 3D object in different views as it is planned to use video-recording robots for perception.
\citet{qi2016volumetric} argued that MVCNNs perform better than volumetric CNNs due to input resolutions.
The 3D representation of an object must be reduced to train a deep learning model in a reasonable timeframe.

Apart from MVCNN, PointNet++ \citep{qi2017pointnet++} or volumetric CNN with 3D Generative Adversarial Network (GAN) \citep{wu2016learning} can be considered for image classification.
The former uses perceived point cloud directly to obtain high-level features from several small local geometric features, whereas the latter performs unsupervised learning where the features created by GAN are used for classification.



\subsubsection{Object Detection}
% Section 6.2
Once classification of scenes and objects is performed,

\subsubsection{Semantic Segmentation}
% Section 6.3


\subsubsection{Object Pose Estimation}
% Section 6.5


\subsection{Navigation}
\subsubsection{Reinforcement Learning}
A reinforcement learning (RL) agent can be trained to move safely in an unperceived environment while maximising the amount of perceived data.


\subsection{3D Reconstruction}
% Section 6.6
3D scan to CAD
% object detection, pose estimation, 3D reconstruction, saliency detection

% Numerous improvements have been made for indoor scene understanding
\subsection{Datasets}
A sufficient amount of labelled images is essential to train a machine learning agent to obtain useful information from perception data.
\citet{silberman2011indoor} and \citet{li2018interiornet, mccormac2016scenenet} has improved indoor scene understanding by providing large-scale real and synthetic indoor image datasets, respectively.

\subsection{Explainable Learning Methods}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Research Design and Methodology}
%%% Details of how the research will be carried out, including any special facilities / resources etc. which would be required and any necessary skills which you either have already or would need to acquire [the tools that will enable us to fill the gap you have identified]

%%% Show three things:
%%% 1. The research is feasible within the amount of time you have.
%%% 2. you can do the research
%%% 3. you are aware of the limitations of your preferred method and have (even if only very briefly) considered other options.

\subsection{Empirical Materials}
\subsubsection{Robot Platform}
TurtleBot with a RGB-D sensor would be sufficient for perception and navigation tasks planned in this project.

\subsubsection{Simulation Environment}
Gazebo can be used

% \subsection{Tasks and Time Allocation}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{landscape}

    \subsection{Research Tasks and Time Allocation}
    %%% A plan and timetable of the work you will carry out
    %%% the feasibility of successfully completing the project in the time available

    \begin{figure}[h]
        \centering
        \includegraphics[scale=0.9]{images/gantt-chart.png}
        \caption{Gantt chart for time allocation of this project.}
        \label{fig:gantt-chart}
    \end{figure}

\end{landscape}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\bibliographystyle{IEEEtranN}
\bibliography{references.bib}

\end{document}
